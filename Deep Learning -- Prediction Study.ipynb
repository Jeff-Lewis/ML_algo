{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BORIS CHANGE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "def get_prices(ticks, st, ed):\n",
    "# This function gets Adjusted Closing prices from Yahoo Finance\n",
    "# returns a DataFrame. Inputs are ticks (list of tickers), st (start date), ed (end date)\n",
    "    for idx, ticker in enumerate(ticks):\n",
    "        print(ticker)\n",
    "        f = web.DataReader(ticker, 'yahoo', st, ed)['Adj Close']\n",
    "        f.name = ticker\n",
    "        if idx==0:\n",
    "            df = f\n",
    "        else:\n",
    "            df = pd.concat([df, f], axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPY', 'AAPL', 'TLT', 'GLD']\n",
      "(1741, 4)\n"
     ]
    }
   ],
   "source": [
    "tickers = [['SPY', 'AAPL','TLT','GLD']]\n",
    "\n",
    "start = dt.datetime(2010, 1, 1)\n",
    "end   = dt.datetime.today()\n",
    "\n",
    "df = get_prices(tickers, start, end)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                AAPL       TLT       GLD       SPY\n",
      "Date                                              \n",
      "2010-01-04  0.000000  0.000000  0.000000  0.000000\n",
      "2010-01-05  0.001729  0.006458 -0.000911  0.002647\n",
      "2010-01-06 -0.015906 -0.013386  0.016500  0.000704\n",
      "2010-01-07 -0.001849  0.001682 -0.006188  0.004221\n",
      "2010-01-08  0.006648 -0.000448  0.004963  0.003328\n",
      "Date\n",
      "2016-11-23    220.699997\n",
      "2016-11-25    221.520004\n",
      "2016-11-28    220.479996\n",
      "2016-11-29    220.910004\n",
      "2016-11-30    220.380005\n",
      "Name: SPY, dtype: float64\n",
      "Date\n",
      "2016-11-23   -1.040008\n",
      "2016-11-25    0.430008\n",
      "2016-11-28   -0.529999\n",
      "2016-11-29    0.000000\n",
      "2016-11-30    0.000000\n",
      "Name: SPY, dtype: float64\n",
      "Positive 0.550, negative 0.450\n",
      "(1741,)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#print(df.head())\n",
    "X = df[['AAPL', 'TLT','GLD','SPY']].pct_change().fillna(0)\n",
    "\n",
    "#print('Cross Correlation')\n",
    "#print(np.corrcoef(X.T)) # careful here need to transpose \"X\" \n",
    "\n",
    "print(X.head())\n",
    "# pull back prices by one day, fill NA forward, 1-day ahead returns\n",
    "y = df['SPY'].shift(-2).fillna(method='ffill') - df['SPY'].shift(-1).fillna(method='ffill') \n",
    "\n",
    "print(df['SPY'].tail())\n",
    "print(y.tail())\n",
    "    \n",
    "y = np.where(y>0, 1, 0) # dummy 1 if return > 0, else 0\n",
    "print('Positive %.3f, negative %.3f' % (y.sum()/len(y), 1-y.sum()/len(y)))\n",
    "\n",
    "print(y.shape)\n",
    "print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive 0.550, negative 0.450\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                train_size = 0.7, random_state = 0, \n",
    "                stratify = y)\n",
    "\n",
    "print('Positive %.3f, negative %.3f' % (y_train.sum()/len(y_train), \n",
    "                                        1-y_train.sum()/len(y_train)))\n",
    "\n",
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.550082\n",
      "Test set score: 0.550669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=30, alpha=1e-4,\n",
    "                    solver='sgd', verbose=False, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.1, activation='logistic') #tanh')\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "              #      ('pca', PCA(n_components=0.75)),\n",
    "                    ('clf', mlp)])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % (pipe_lr.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold stratified cross validation accuracy scores: [ 0.54918033  0.54918033  0.54918033  0.54918033  0.54918033  0.54918033\n",
      "  0.54918033  0.54918033  0.55371901  0.55371901]\n",
      "CV accuracy: 0.550 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # doees stratified k-fold cross validation \n",
    "\n",
    "scores = cross_val_score(estimator = pipe_lr,\n",
    "                        X=X_train, \n",
    "                        y=y_train,\n",
    "                        cv = 10)\n",
    "\n",
    "print('k-fold stratified cross validation accuracy scores: %s' %scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.550082101806\n",
      "{'clf__activation': 'relu', 'clf__solver': 'sgd', 'clf__hidden_layer_sizes': (100, 100), 'clf__alpha': 1.0}\n",
      "Test accuracy: 0.551\n"
     ]
    }
   ],
   "source": [
    "# find best hyper-parameters for Deep Learning Neural Network\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_mlp = Pipeline([('scl', StandardScaler()),\n",
    "                     ('clf', mlp)])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "\n",
    "param_grid = [{'clf__alpha'     : param_range, # regularization strength on L2\n",
    "               'clf__activation': ['relu','logistic', 'tanh'],\n",
    "               'clf__solver'    : ['sgd'],\n",
    "               'clf__hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "    \n",
    "gs = GridSearchCV(estimator = pipe_mlp,\n",
    "                  param_grid = param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv = 2,\n",
    "                  n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_mlp = gs.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % best_mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000000\n",
      "Test set score: 0.500956\n",
      "0.50328407225\n",
      "{'clf__n_estimators': 100, 'clf__criterion': 'gini'}\n",
      "Test accuracy: 0.520\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % rf.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % rf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "pipe_rf = Pipeline([    #('scl', StandardScaler()),\n",
    "                     ('clf', rf)])\n",
    "\n",
    "param_range = [100,200,300]\n",
    "\n",
    "param_grid = [{'clf__n_estimators' : [100,200,300], # nos of trees in the forest\n",
    "               'clf__criterion': ['gini', 'entropy']}]\n",
    "    \n",
    "gs = GridSearchCV(estimator = pipe_rf,\n",
    "                  param_grid = param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv = 2,\n",
    "                  n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_rf = gs.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % best_rf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1.0, kernel='rbf') # ‘linear’ or 'rbf'\n",
    "        \n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % svc.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(X), type(y))\n",
    "print(X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Perceptron\",\n",
    "         \"LogisticRegression\",\n",
    "         \"Linear SVM\", \n",
    "         \"Decision Tree\", \n",
    "         \"Random Forest\", \n",
    "         \"AdaBoost\",\n",
    "         \"RBF SVM\",          \n",
    "         \"Neural Net\", \n",
    "         \"Naive Bayes\",  \n",
    "         \"Nearest Neighbors\"] \n",
    "\n",
    "classifiers = [\n",
    "    Perceptron(),\n",
    "    LogisticRegression(),\n",
    "    SVC(kernel=\"linear\", C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel='rbf', gamma=2, C=1),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,1000), alpha=1),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(5)]\n",
    "\n",
    "datasets = [(X, y)]             \n",
    "\n",
    "figure = plt.figure(figsize=(30, 10))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    XX, yy = ds\n",
    "    XX = StandardScaler().fit_transform(XX)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(XX, yy, test_size=.3, random_state=42)\n",
    "\n",
    "    x_min, x_max = XX[:, 0].min() - .5, XX[:, 0].max() + .5\n",
    "    y_min, y_max = XX[:, 1].min() - .5, XX[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.3f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep learning has caught the attention and fear of many investors. Applications of using neural networks\n",
    "# has be employed to successfully best humans in chess, Jeporedy, and recently Go. Does this mean that \n",
    "# it could be possible to create an artificial intelligent machine to succesfully trade against human?\n",
    "# In this study we attempt to gain an understanding of how powerful the popular machine learning algos are\n",
    "# that are readily available to even the lay person. \n",
    "# We ask the question, can we\n",
    "\n",
    "# Neural networks are built from Single Layer Perceptrons by stacking the perceptrons. In our model we use a two hidden \n",
    "# neural network also know as a deep learning neural network. We impose L2 regularization to mitigate high variance\n",
    "# in our model. \n",
    "\n",
    "# Table\n",
    "# Algorithm, 10-k cross validation score +/- std, best hyper-parameters, and regularization (L1, L2, or Elastic Net)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
