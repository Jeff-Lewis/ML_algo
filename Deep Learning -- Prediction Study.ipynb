{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader import data, wb\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prices(tickers_list, start, end, what_price):\n",
    "    df = pd.DataFrame()\n",
    "    for ticker in tickers_list:\n",
    "        tmp = data.DataReader(ticker, 'yahoo', start, end)\n",
    "        df[ticker] = tmp[what_price]\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is pct_change good enough? Maybe make it ln(x2/x1)?\n",
    "#df.plot()\n",
    "#plt.legend(loc='lower left')\n",
    "#plt.show()\n",
    "#print(np.corrcoef(X.T)) # cross-correlation careful here need to transpose \"X\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#######################\n",
    "tickers = ['SPY', 'TLT', 'JNK', 'IYR', 'FXE']# , 'GSG', 'EEM', 'IWM', 'HYG', 'UNG', 'LQD']\n",
    "st = dt.datetime(2010, 1, 1)\n",
    "ed = dt.datetime(2016, 1, 1)\n",
    "prior_days_ago    = 1    # nos of days ago -- pct_change ago\n",
    "next_days_forward = [10, 60, 120]#, 20, 40, 60, 90] # nos of days forward -- pct_change\n",
    "#######################\n",
    "\n",
    "df = get_prices(tickers, st, ed, 'Adj Close')\n",
    "in_ = True\n",
    "\n",
    "#for idx, tick in enumerate(tickers):\n",
    "idx = 0\n",
    "tick = 'SPY'\n",
    "\n",
    "for n_fwd in next_days_forward:\n",
    "\n",
    "    prior_n = n_fwd\n",
    "\n",
    "    X = df[ tickers ].pct_change(prior_n).fillna(0) # 1 day ago returns, 2-days ago return\n",
    "\n",
    "    ## add dummies for days of the week ##\n",
    "    X['wk_day'] = X.index.weekday_name;   X = pd.get_dummies(X)\n",
    "    X['month']  = X.index.strftime('%b');   X = pd.get_dummies(X)\n",
    "    ## add dummies for days of the week ##\n",
    "\n",
    "    y = df[tick].shift(-n_fwd).fillna(method='ffill') / df[tick].shift(0).fillna(method='ffill') - 1\n",
    "    y = np.where(y>0, 1, 0) # dummy 1 if return > 0, else 0\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                train_size = 0.7, random_state = 0)\n",
    "\n",
    "    ##### added pipeline #### \n",
    "    pipe_mlp = Pipeline([('scl', StandardScaler()),\n",
    "                 ('clf', mlp)])\n",
    "\n",
    "    #param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    param_range = [0.01, 10.0]\n",
    "\n",
    "    param_grid = [{'clf__alpha'     : param_range, # regularization strength on L2\n",
    "                   'clf__activation': ['relu'], #,'logistic', 'tanh'],\n",
    "                   'clf__solver'    : ['sgd'],\n",
    "                   'clf__hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "\n",
    "    gs = GridSearchCV(estimator = pipe_mlp,\n",
    "                      param_grid = param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv = 3,\n",
    "                      n_jobs = -1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "\n",
    "    best_mlp = gs.best_estimator_\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "\n",
    "    train_score = best_mlp.score(X_train, y_train)\n",
    "    test_score  = best_mlp.score(X_test, y_test)\n",
    "\n",
    "#        train_score = f1_score(y_train, best_mlp.predict(X_train) ) \n",
    "#        test_score  = f1_score(y_test , best_mlp.predict(X_test)  )\n",
    "\n",
    "\n",
    "    if in_:\n",
    "        print('Ticker, Base%, TrainScore, TestScore, prior_days_ago, next_days_forward ')\n",
    "        print(tick,',%.3f, %.3f, %.3f, %.f, %.f' % (y.sum()/len(y), train_score, test_score, prior_n, n_fwd)) \n",
    "        in_ = False\n",
    "    else:\n",
    "        print(tick,',%.3f, %.3f, %.3f, %.f, %.f' % (y.sum()/len(y), train_score, test_score, prior_n, n_fwd))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker, Base%, TrainScore, TestScore, prior_days_ago, next_days_forward \n",
      "SPY ,0.552, 0.716, 0.700, 1, 1\n",
      "SPY ,0.599, 0.770, 0.737, 5, 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b9fe583d92e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                       n_jobs = -1)\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#    print(gs.best_score_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \"\"\"\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    560\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 562\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m           for train, test in cv.split(X, y, groups))\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/anaconda/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING \n",
    "# For first try, assign SPY as dependent variable an lag 1 day returns as explanatory variables\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#######################\n",
    "tickers = ['SPY', 'TLT', 'JNK', 'IYR', 'FXE', 'GSG', 'EEM'] #, 'IWM', 'HYG', 'UNG', 'LQD']\n",
    "st = dt.datetime(2010, 1, 1)\n",
    "ed = dt.datetime(2016, 1, 1)\n",
    "prior_days_ago    = 1    # nos of days ago -- pct_change ago\n",
    "next_days_forward = [1, 5, 20] #, 20, 40, 60, 90] # nos of days forward -- pct_change\n",
    "#######################\n",
    "\n",
    "df = get_prices(tickers, st, ed, 'Adj Close')\n",
    "in_ = True\n",
    "\n",
    "#for idx, tick in enumerate(tickers):\n",
    "idx = 0\n",
    "tick = 'SPY'\n",
    "\n",
    "for n_fwd in next_days_forward:\n",
    "\n",
    "    prior_n = n_fwd\n",
    "\n",
    "    X = df[ tickers ].pct_change(prior_n).fillna(0) # 1 day ago returns, 2-days ago return\n",
    "\n",
    "    ## add dummies for days of the week & month of year, what about first Friday of month? ##\n",
    "    X['wk_day'] = X.index.weekday_name;     X = pd.get_dummies(X)\n",
    "    X['month']  = X.index.strftime('%b');   X = pd.get_dummies(X)\n",
    "    ## add dummies for days of the week ##\n",
    "\n",
    "    y = df[tick].shift(-n_fwd).fillna(method='ffill') / df[tick].shift(0).fillna(method='ffill') - 1\n",
    "    y = np.where(y>0, 1, 0) # dummy 1 if return > 0, else 0\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                train_size = 0.7, random_state = 0)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    ### Classifier ### \n",
    "    mlp = MLPClassifier()\n",
    "\n",
    "    #param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    param_range = [0.01, 10.0]\n",
    "\n",
    "    param_grid = [{'alpha'     : param_range, # regularization strength on L2\n",
    "                   'activation': ['relu'], #,'logistic', 'tanh'],\n",
    "                   'solver'    : ['sgd'],\n",
    "                   'hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "\n",
    "    gs = GridSearchCV(estimator = mlp,\n",
    "                      param_grid = param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv = 3,\n",
    "                      n_jobs = -1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "\n",
    "    #    print(gs.best_score_)\n",
    "    #    print(gs.best_params_)\n",
    "\n",
    "    best_mlp = gs.best_estimator_\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "\n",
    "    train_score = best_mlp.score(X_train, y_train)\n",
    "    test_score  = best_mlp.score(X_test, y_test)\n",
    "\n",
    "    train_score = f1_score(y_train, best_mlp.predict(X_train) ) \n",
    "    test_score  = f1_score(y_test , best_mlp.predict(X_test)  )\n",
    "\n",
    "\n",
    "    if in_:\n",
    "        print('Ticker, Base%, TrainScore, TestScore, prior_days_ago, next_days_forward ')\n",
    "        print(tick,',%.3f, %.3f, %.3f, %.f, %.f' % (y.sum()/len(y), train_score, test_score, prior_n, n_fwd)) \n",
    "        in_ = False\n",
    "    else:\n",
    "        print(tick,',%.3f, %.3f, %.3f, %.f, %.f' % (y.sum()/len(y), train_score, test_score, prior_n, n_fwd))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "#param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_range = [0.01, 10.0]\n",
    "\n",
    "\n",
    "param_grid = [{'alpha'     : param_range, # regularization strength on L2\n",
    "           'activation': ['relu'], #,'logistic', 'tanh'],\n",
    "           'solver'    : ['sgd'],\n",
    "           'hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "\n",
    "gs = GridSearchCV(estimator = mlp,\n",
    "              param_grid = param_grid,\n",
    "              scoring='accuracy',\n",
    "              cv = 2,\n",
    "              n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_mlp = gs.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "print('Train accuracy: %.3f' % best_mlp.score(X_train, y_train))\n",
    "print('Test  accuracy: %.3f' % best_mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adsf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "def get_prices(ticks, st, ed):\n",
    "# This function gets Adjusted Closing prices from Yahoo Finance\n",
    "# returns a DataFrame. Inputs are ticks (list of tickers), st (start date), ed (end date)\n",
    "    for idx, ticker in enumerate(ticks):\n",
    "        print(ticker)\n",
    "        f = web.DataReader(ticker, 'yahoo', st, ed)['Adj Close']\n",
    "        f.name = ticker\n",
    "        if idx==0:\n",
    "            df = f\n",
    "        else:\n",
    "            df = pd.concat([df, f], axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Jimtickers = [['SPY', 'AAPL','TLT','GLD']]\n",
    "\n",
    "start = dt.datetime(2010, 1, 1)\n",
    "end   = dt.datetime.today()\n",
    "\n",
    "df = get_prices(tickers, start, end)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df.head())\n",
    "X = df[['AAPL', 'TLT','GLD','SPY']].pct_change().fillna(0)\n",
    "\n",
    "#print('Cross Correlation')\n",
    "#print(np.corrcoef(X.T)) # careful here need to transpose \"X\" \n",
    "\n",
    "print(X.head())\n",
    "# pull back prices by one day, fill NA forward, 1-day ahead returns\n",
    "y = df['SPY'].shift(-2).fillna(method='ffill') - df['SPY'].shift(-1).fillna(method='ffill') \n",
    "\n",
    "print(df['SPY'].tail())\n",
    "print(y.tail())\n",
    "    \n",
    "y = np.where(y>0, 1, 0) # dummy 1 if return > 0, else 0\n",
    "print('Positive %.3f, negative %.3f' % (y.sum()/len(y), 1-y.sum()/len(y)))\n",
    "\n",
    "print(y.shape)\n",
    "print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                train_size = 0.7, random_state = 0, \n",
    "                stratify = y)\n",
    "\n",
    "print('Positive %.3f, negative %.3f' % (y_train.sum()/len(y_train), \n",
    "                                        1-y_train.sum()/len(y_train)))\n",
    "\n",
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=30, alpha=1e-4,\n",
    "                    solver='sgd', verbose=False, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.1, activation='logistic') #tanh')\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "              #      ('pca', PCA(n_components=0.75)),\n",
    "                    ('clf', mlp)])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % (pipe_lr.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score # doees stratified k-fold cross validation \n",
    "\n",
    "scores = cross_val_score(estimator = pipe_lr,\n",
    "                        X=X_train, \n",
    "                        y=y_train,\n",
    "                        cv = 10)\n",
    "\n",
    "print('k-fold stratified cross validation accuracy scores: %s' %scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find best hyper-parameters for Deep Learning Neural Network\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_mlp = Pipeline([('scl', StandardScaler()),\n",
    "                     ('clf', mlp)])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "\n",
    "param_grid = [{'clf__alpha'     : param_range, # regularization strength on L2\n",
    "               'clf__activation': ['relu','logistic', 'tanh'],\n",
    "               'clf__solver'    : ['sgd'],\n",
    "               'clf__hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "    \n",
    "gs = GridSearchCV(estimator = pipe_mlp,\n",
    "                  param_grid = param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv = 2,\n",
    "                  n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_mlp = gs.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "print('Test accuracy: %.6f' % best_mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % rf.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % rf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "pipe_rf = Pipeline([    #('scl', StandardScaler()),\n",
    "                     ('clf', rf)])\n",
    "\n",
    "param_range = [100,200,300]\n",
    "\n",
    "param_grid = [{'clf__n_estimators' : [100,200,300], # nos of trees in the forest\n",
    "               'clf__criterion': ['gini', 'entropy']}]\n",
    "    \n",
    "gs = GridSearchCV(estimator = pipe_rf,\n",
    "                  param_grid = param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv = 2,\n",
    "                  n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_rf = gs.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Test accuracy: %.3f' % best_rf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1.0, kernel='rbf') # ‘linear’ or 'rbf'\n",
    "        \n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % svc.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(X), type(y))\n",
    "print(X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Perceptron\",\n",
    "         \"LogisticRegression\",\n",
    "         \"Linear SVM\", \n",
    "         \"Decision Tree\", \n",
    "         \"Random Forest\", \n",
    "         \"AdaBoost\",\n",
    "         \"RBF SVM\",          \n",
    "         \"Neural Net\", \n",
    "         \"Naive Bayes\",  \n",
    "         \"Nearest Neighbors\"] \n",
    "\n",
    "classifiers = [\n",
    "    Perceptron(),\n",
    "    LogisticRegression(),\n",
    "    SVC(kernel=\"linear\", C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel='rbf', gamma=2, C=1),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,1000), alpha=1),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(5)]\n",
    "\n",
    "datasets = [(X, y)]             \n",
    "\n",
    "figure = plt.figure(figsize=(30, 10))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    XX, yy = ds\n",
    "    XX = StandardScaler().fit_transform(XX)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(XX, yy, test_size=.3, random_state=42)\n",
    "\n",
    "    x_min, x_max = XX[:, 0].min() - .5, XX[:, 0].max() + .5\n",
    "    y_min, y_max = XX[:, 1].min() - .5, XX[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.3f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep learning has caught the attention and fear of many investors. Applications of using neural networks\n",
    "# has be employed to successfully best humans in chess, Jeporedy, and recently Go. Does this mean that \n",
    "# it could be possible to create an artificial intelligent machine to succesfully trade against human?\n",
    "# In this study we attempt to gain an understanding of how powerful the popular machine learning algos are\n",
    "# that are readily available to even the lay person. \n",
    "# We ask the question, can we\n",
    "\n",
    "# Neural networks are built from Single Layer Perceptrons by stacking the perceptrons. In our model we use a two hidden \n",
    "# neural network also know as a deep learning neural network. We impose L2 regularization to mitigate high variance\n",
    "# in our model. \n",
    "\n",
    "# Table\n",
    "# Algorithm, 10-k cross validation score +/- std, best hyper-parameters, and regularization (L1, L2, or Elastic Net)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) What is Machine Learning? \n",
    "\n",
    "# 2) Label each below with either a \"S\" supervised learning algo, \"U\" unsupervised learning algo or \"Neither\" \n",
    "AdaBoost # \"S\" Classifier(), \n",
    "Decision Tree # \"S\" Classifier(max_depth=5),\n",
    "Density-based Spatial Clustering of Applications with Noise (DBSCAN) , # Unsupervised-Classifier\n",
    "Flask # \"Neither\" \n",
    "IRIS # \"Neither\" \n",
    "K-Fold Cross Validation # \"Neither\" \n",
    "K-Nearest Neighbors # \"S\" Classifier(5)]\n",
    "K-means++ # Unsupervised Classifier \n",
    "Linear Discriminant Analysis # \"S\" supervised \n",
    "Linear Regression # \"S\" Regression \n",
    "Logistic Regression # \"S\" Classifier (),\n",
    "Multi-Layer Perceptron # \"S\" Classifier(hidden_layer_sizes=(100,1000), alpha=1), and Regression\n",
    "MNIST # \"Neither\"\n",
    "Normalization # \"Neither\"\n",
    "Pandas # \"Neither\" \n",
    "Perceptron # \"S\", Classifier (),\n",
    "Principal Component Analysis # \"U\"\n",
    "Random Forest # \"S\", Classifier(n_estimators=100), Regressor\n",
    "RANdom SAmple Consensus (RANSAC) # \"S\",  Supervised Regression\n",
    "SciKit-Learn  # \"Neither\" \n",
    "Standardization # \"Neither\" \n",
    "Stochastic Gradient Descent # \"Neither\"\n",
    "Support Vector Machine # \"S\", Classifier \n",
    "\n",
    "# 3) \n",
    "Match the graphs below\n",
    "\n",
    "# 4) Given the Confusion Matrix compute:\n",
    "# precision, recall, and f1-score = 2*(PRExREC)/(PRE+REC) \n",
    "\n",
    "# 5) Write the whole equations for regularized linear regressions:\n",
    "# a) Ridge Regression = \n",
    "# b) Least Absolute Shrinkage and Selection Ooperator\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
