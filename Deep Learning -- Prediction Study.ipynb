{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader import data, wb\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prices(tickers_list, start, end, what_price):\n",
    "    df = pd.DataFrame()\n",
    "    for ticker in tickers_list:\n",
    "        tmp = data.DataReader(ticker, 'yahoo', start, end)\n",
    "        df[ticker] = tmp[what_price]\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is pct_change good enough? Maybe make it ln(x2/x1)?\n",
    "#df.plot()\n",
    "#plt.legend(loc='lower left')\n",
    "#plt.show()\n",
    "#print(np.corrcoef(X.T)) # cross-correlation careful here need to transpose \"X\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker, Base%, TrainScore, TestScore, prior_days_ago, next_days_forward \n",
      "SPY ,0.548, 0.553, 0.541, 1, 1\n",
      "SPY ,0.578, 0.652, 0.593, 5, 5\n",
      "SPY ,0.611, 0.675, 0.628, 10, 10\n",
      "SPY ,0.646, 0.725, 0.688, 20, 20\n",
      "SPY ,0.670, 0.774, 0.734, 40, 40\n",
      "SPY ,0.684, 0.862, 0.783, 60, 60\n",
      "SPY ,0.698, 0.846, 0.820, 90, 90\n",
      "TLT ,0.524, 0.518, 0.537, 1, 1\n",
      "TLT ,0.540, 0.652, 0.574, 5, 5\n",
      "TLT ,0.539, 0.674, 0.607, 10, 10\n",
      "TLT ,0.543, 0.702, 0.656, 20, 20\n",
      "TLT ,0.597, 0.776, 0.697, 40, 40\n",
      "TLT ,0.600, 0.810, 0.720, 60, 60\n",
      "TLT ,0.600, 0.842, 0.813, 90, 90\n",
      "JNK ,0.419, 0.587, 0.569, 1, 1\n",
      "JNK ,0.470, 0.652, 0.577, 5, 5\n",
      "JNK ,0.484, 0.684, 0.615, 10, 10\n",
      "JNK ,0.493, 0.734, 0.664, 20, 20\n",
      "JNK ,0.520, 0.872, 0.815, 40, 40\n",
      "JNK ,0.539, 0.918, 0.873, 60, 60\n",
      "JNK ,0.528, 0.943, 0.901, 90, 90\n",
      "IYR ,0.522, 0.547, 0.493, 1, 1\n",
      "IYR ,0.566, 0.639, 0.582, 5, 5\n",
      "IYR ,0.577, 0.671, 0.615, 10, 10\n",
      "IYR ,0.608, 0.735, 0.684, 20, 20\n",
      "IYR ,0.614, 0.808, 0.762, 40, 40\n",
      "IYR ,0.647, 0.823, 0.782, 60, 60\n",
      "IYR ,0.674, 0.908, 0.870, 90, 90\n",
      "FXE ,0.504, 0.508, 0.489, 1, 1\n",
      "FXE ,0.499, 0.571, 0.560, 5, 5\n",
      "FXE ,0.503, 0.667, 0.620, 10, 10\n",
      "FXE ,0.509, 0.709, 0.631, 20, 20\n",
      "FXE ,0.549, 0.778, 0.722, 40, 40\n",
      "FXE ,0.565, 0.822, 0.767, 60, 60\n",
      "FXE ,0.563, 0.903, 0.831, 90, 90\n",
      "GSG ,0.466, 0.538, 0.526, 1, 1\n",
      "GSG ,0.470, 0.539, 0.519, 5, 5\n",
      "GSG ,0.466, 0.667, 0.563, 10, 10\n",
      "GSG ,0.459, 0.722, 0.651, 20, 20\n",
      "GSG ,0.471, 0.783, 0.716, 40, 40\n",
      "GSG ,0.464, 0.839, 0.792, 60, 60\n",
      "GSG ,0.443, 0.890, 0.829, 90, 90\n",
      "EEM ,0.518, 0.596, 0.493, 1, 1\n",
      "EEM ,0.527, 0.660, 0.594, 5, 5\n",
      "EEM ,0.547, 0.665, 0.603, 10, 10\n",
      "EEM ,0.532, 0.748, 0.657, 20, 20\n",
      "EEM ,0.551, 0.742, 0.725, 40, 40\n",
      "EEM ,0.575, 0.814, 0.738, 60, 60\n",
      "EEM ,0.544, 0.822, 0.766, 90, 90\n",
      "IWM ,0.534, 0.530, 0.541, 1, 1\n",
      "IWM ,0.557, 0.555, 0.571, 5, 5\n",
      "IWM ,0.576, 0.683, 0.628, 10, 10\n",
      "IWM ,0.599, 0.690, 0.656, 20, 20\n",
      "IWM ,0.634, 0.768, 0.729, 40, 40\n",
      "IWM ,0.636, 0.823, 0.772, 60, 60\n",
      "IWM ,0.626, 0.851, 0.804, 90, 90\n",
      "HYG ,0.460, 0.547, 0.530, 1, 1\n",
      "HYG ,0.504, 0.581, 0.536, 5, 5\n",
      "HYG ,0.518, 0.673, 0.624, 10, 10\n",
      "HYG ,0.531, 0.731, 0.677, 20, 20\n",
      "HYG ,0.556, 0.844, 0.791, 40, 40\n",
      "HYG ,0.567, 0.900, 0.872, 60, 60\n",
      "HYG ,0.574, 0.914, 0.857, 90, 90\n",
      "UNG ,0.406, 0.604, 0.571, 1, 1\n",
      "UNG ,0.392, 0.622, 0.575, 5, 5\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING \n",
    "# For first try, assign SPY as dependent variable an lag 1 day returns as explanatory variables\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#######################\n",
    "tickers = ['SPY', 'TLT', 'JNK', 'IYR', 'FXE', 'GSG', 'EEM', 'IWM', 'HYG', 'UNG', 'LQD']\n",
    "st = dt.datetime(2006, 1, 1)\n",
    "ed = dt.datetime(2016, 1, 1)\n",
    "prior_days_ago    = 1    # nos of days ago -- pct_change ago\n",
    "next_days_forward = [1, 5, 10, 20, 40, 60, 90] # nos of days forward -- pct_change\n",
    "#######################\n",
    "\n",
    "df = get_prices(tickers, st, ed, 'Adj Close')\n",
    "in_ = True\n",
    "\n",
    "for idx, tick in enumerate(tickers):\n",
    "    \n",
    "    for n_fwd in next_days_forward:\n",
    "        \n",
    "        prior_n = n_fwd\n",
    "        \n",
    "        X = df[ tickers ].pct_change(prior_n).fillna(0) # 1 day ago returns, 2-days ago return\n",
    "        \n",
    "        ## add dummies for days of the week ##\n",
    "        X['wk_day'] = X.index.weekday_name;   X = pd.get_dummies(X)\n",
    "        X['month']  = X.index.strftime('%b');   X = pd.get_dummies(X)\n",
    "        ## add dummies for days of the week ##\n",
    "        \n",
    "        y = df[tick].shift(-n_fwd).fillna(method='ffill') / df[tick].shift(0).fillna(method='ffill') - 1\n",
    "        y = np.where(y>0, 1, 0) # dummy 1 if return > 0, else 0\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                    train_size = 0.7, random_state = 0)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "\n",
    "        ### Classifier ### \n",
    "        mlp = MLPClassifier()\n",
    "\n",
    "        #param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "        param_range = [0.01, 10.0]\n",
    "\n",
    "        param_grid = [{'alpha'     : param_range, # regularization strength on L2\n",
    "                       'activation': ['relu'], #,'logistic', 'tanh'],\n",
    "                       'solver'    : ['sgd'],\n",
    "                       'hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "\n",
    "        gs = GridSearchCV(estimator = mlp,\n",
    "                          param_grid = param_grid,\n",
    "                          scoring='accuracy',\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "        gs = gs.fit(X_train, y_train)\n",
    "\n",
    "    #    print(gs.best_score_)\n",
    "    #    print(gs.best_params_)\n",
    "\n",
    "        best_mlp = gs.best_estimator_\n",
    "        best_mlp.fit(X_train, y_train)\n",
    "\n",
    "        train_score = best_mlp.score(X_train, y_train)\n",
    "        test_score  = best_mlp.score(X_test, y_test)\n",
    "\n",
    "        train_score = f1_score(y_train, best_mlp.predict(X_train) ) \n",
    "        test_score  = f1_score(y_test , best_mlp.predict(X_test)  )\n",
    "        \n",
    "        \n",
    "        if in_:\n",
    "            print('Ticker, Base%, TrainScore, TestScore, prior_days_ago, next_days_forward ')\n",
    "            print(tick,',%.3f, %.3f, %.3f, %.f, %.f' % (y.sum()/len(y), train_score, test_score, prior_n, n_fwd)) \n",
    "            in_ = False\n",
    "        else:\n",
    "            print(tick,',%.3f, %.3f, %.3f, %.f, %.f' % (y.sum()/len(y), train_score, test_score, prior_n, n_fwd))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X['month'] = X.index.strftime('%b')\n",
    "X = pd.get_dummies(X)\n",
    "    \n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "#param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_range = [0.01, 10.0]\n",
    "\n",
    "\n",
    "param_grid = [{'alpha'     : param_range, # regularization strength on L2\n",
    "           'activation': ['relu'], #,'logistic', 'tanh'],\n",
    "           'solver'    : ['sgd'],\n",
    "           'hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "\n",
    "gs = GridSearchCV(estimator = mlp,\n",
    "              param_grid = param_grid,\n",
    "              scoring='accuracy',\n",
    "              cv = 2,\n",
    "              n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_mlp = gs.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "print('Train accuracy: %.3f' % best_mlp.score(X_train, y_train))\n",
    "print('Test  accuracy: %.3f' % best_mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adsf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "def get_prices(ticks, st, ed):\n",
    "# This function gets Adjusted Closing prices from Yahoo Finance\n",
    "# returns a DataFrame. Inputs are ticks (list of tickers), st (start date), ed (end date)\n",
    "    for idx, ticker in enumerate(ticks):\n",
    "        print(ticker)\n",
    "        f = web.DataReader(ticker, 'yahoo', st, ed)['Adj Close']\n",
    "        f.name = ticker\n",
    "        if idx==0:\n",
    "            df = f\n",
    "        else:\n",
    "            df = pd.concat([df, f], axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Jimtickers = [['SPY', 'AAPL','TLT','GLD']]\n",
    "\n",
    "start = dt.datetime(2010, 1, 1)\n",
    "end   = dt.datetime.today()\n",
    "\n",
    "df = get_prices(tickers, start, end)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df.head())\n",
    "X = df[['AAPL', 'TLT','GLD','SPY']].pct_change().fillna(0)\n",
    "\n",
    "#print('Cross Correlation')\n",
    "#print(np.corrcoef(X.T)) # careful here need to transpose \"X\" \n",
    "\n",
    "print(X.head())\n",
    "# pull back prices by one day, fill NA forward, 1-day ahead returns\n",
    "y = df['SPY'].shift(-2).fillna(method='ffill') - df['SPY'].shift(-1).fillna(method='ffill') \n",
    "\n",
    "print(df['SPY'].tail())\n",
    "print(y.tail())\n",
    "    \n",
    "y = np.where(y>0, 1, 0) # dummy 1 if return > 0, else 0\n",
    "print('Positive %.3f, negative %.3f' % (y.sum()/len(y), 1-y.sum()/len(y)))\n",
    "\n",
    "print(y.shape)\n",
    "print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                train_size = 0.7, random_state = 0, \n",
    "                stratify = y)\n",
    "\n",
    "print('Positive %.3f, negative %.3f' % (y_train.sum()/len(y_train), \n",
    "                                        1-y_train.sum()/len(y_train)))\n",
    "\n",
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=30, alpha=1e-4,\n",
    "                    solver='sgd', verbose=False, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.1, activation='logistic') #tanh')\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "              #      ('pca', PCA(n_components=0.75)),\n",
    "                    ('clf', mlp)])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % (pipe_lr.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score # doees stratified k-fold cross validation \n",
    "\n",
    "scores = cross_val_score(estimator = pipe_lr,\n",
    "                        X=X_train, \n",
    "                        y=y_train,\n",
    "                        cv = 10)\n",
    "\n",
    "print('k-fold stratified cross validation accuracy scores: %s' %scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find best hyper-parameters for Deep Learning Neural Network\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_mlp = Pipeline([('scl', StandardScaler()),\n",
    "                     ('clf', mlp)])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "\n",
    "param_grid = [{'clf__alpha'     : param_range, # regularization strength on L2\n",
    "               'clf__activation': ['relu','logistic', 'tanh'],\n",
    "               'clf__solver'    : ['sgd'],\n",
    "               'clf__hidden_layer_sizes' : [(100,100), (100,100,100)]}] # 'lbfgs','adam'\n",
    "    \n",
    "gs = GridSearchCV(estimator = pipe_mlp,\n",
    "                  param_grid = param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv = 2,\n",
    "                  n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_mlp = gs.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "print('Test accuracy: %.6f' % best_mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % rf.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % rf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "pipe_rf = Pipeline([    #('scl', StandardScaler()),\n",
    "                     ('clf', rf)])\n",
    "\n",
    "param_range = [100,200,300]\n",
    "\n",
    "param_grid = [{'clf__n_estimators' : [100,200,300], # nos of trees in the forest\n",
    "               'clf__criterion': ['gini', 'entropy']}]\n",
    "    \n",
    "gs = GridSearchCV(estimator = pipe_rf,\n",
    "                  param_grid = param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv = 2,\n",
    "                  n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "best_rf = gs.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Test accuracy: %.3f' % best_rf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1.0, kernel='rbf') # ‘linear’ or 'rbf'\n",
    "        \n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % svc.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(X), type(y))\n",
    "print(X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Perceptron\",\n",
    "         \"LogisticRegression\",\n",
    "         \"Linear SVM\", \n",
    "         \"Decision Tree\", \n",
    "         \"Random Forest\", \n",
    "         \"AdaBoost\",\n",
    "         \"RBF SVM\",          \n",
    "         \"Neural Net\", \n",
    "         \"Naive Bayes\",  \n",
    "         \"Nearest Neighbors\"] \n",
    "\n",
    "classifiers = [\n",
    "    Perceptron(),\n",
    "    LogisticRegression(),\n",
    "    SVC(kernel=\"linear\", C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel='rbf', gamma=2, C=1),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,1000), alpha=1),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(5)]\n",
    "\n",
    "datasets = [(X, y)]             \n",
    "\n",
    "figure = plt.figure(figsize=(30, 10))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    XX, yy = ds\n",
    "    XX = StandardScaler().fit_transform(XX)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(XX, yy, test_size=.3, random_state=42)\n",
    "\n",
    "    x_min, x_max = XX[:, 0].min() - .5, XX[:, 0].max() + .5\n",
    "    y_min, y_max = XX[:, 1].min() - .5, XX[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.3f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep learning has caught the attention and fear of many investors. Applications of using neural networks\n",
    "# has be employed to successfully best humans in chess, Jeporedy, and recently Go. Does this mean that \n",
    "# it could be possible to create an artificial intelligent machine to succesfully trade against human?\n",
    "# In this study we attempt to gain an understanding of how powerful the popular machine learning algos are\n",
    "# that are readily available to even the lay person. \n",
    "# We ask the question, can we\n",
    "\n",
    "# Neural networks are built from Single Layer Perceptrons by stacking the perceptrons. In our model we use a two hidden \n",
    "# neural network also know as a deep learning neural network. We impose L2 regularization to mitigate high variance\n",
    "# in our model. \n",
    "\n",
    "# Table\n",
    "# Algorithm, 10-k cross validation score +/- std, best hyper-parameters, and regularization (L1, L2, or Elastic Net)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) What is Machine Learning? \n",
    "\n",
    "# 2) Label each below with either a \"S\" supervised learning algo, \"U\" unsupervised learning algo or \"Neither\" \n",
    "AdaBoost # \"S\" Classifier(), \n",
    "Decision Tree # \"S\" Classifier(max_depth=5),\n",
    "Density-based Spatial Clustering of Applications with Noise (DBSCAN) , # Unsupervised-Classifier\n",
    "Flask # \"Neither\" \n",
    "IRIS # \"Neither\" \n",
    "K-Fold Cross Validation # \"Neither\" \n",
    "K-Nearest Neighbors # \"S\" Classifier(5)]\n",
    "K-means++ # Unsupervised Classifier \n",
    "Linear Discriminant Analysis # \"S\" supervised \n",
    "Linear Regression # \"S\" Regression \n",
    "Logistic Regression # \"S\" Classifier (),\n",
    "Multi-Layer Perceptron # \"S\" Classifier(hidden_layer_sizes=(100,1000), alpha=1), and Regression\n",
    "MNIST # \"Neither\"\n",
    "Normalization # \"Neither\"\n",
    "Pandas # \"Neither\" \n",
    "Perceptron # \"S\", Classifier (),\n",
    "Principal Component Analysis # \"U\"\n",
    "Random Forest # \"S\", Classifier(n_estimators=100), Regressor\n",
    "RANdom SAmple Consensus (RANSAC) # \"S\",  Supervised Regression\n",
    "SciKit-Learn  # \"Neither\" \n",
    "Standardization # \"Neither\" \n",
    "Stochastic Gradient Descent # \"Neither\"\n",
    "Support Vector Machine # \"S\", Classifier \n",
    "\n",
    "# 3) \n",
    "Match the graphs below\n",
    "\n",
    "# 4) Given the Confusion Matrix compute:\n",
    "# precision, recall, and f1-score = 2*(PRExREC)/(PRE+REC) \n",
    "\n",
    "# 5) Write the whole equations for regularized linear regressions:\n",
    "# a) Ridge Regression = \n",
    "# b) Least Absolute Shrinkage and Selection Ooperator\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
